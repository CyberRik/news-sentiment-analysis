import requests
from bs4 import BeautifulSoup
import datetime
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

def get_all_article_dates(page_number):
    url = f'https://markets.businessinsider.com/news/aapl-stock?p={page_number}'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')

    articles = soup.find_all('div', class_='latest-news__story')
    article_dates = []

    for article in articles:
        date_str = article.find('time', class_='latest-news__date').get('datetime')
        article_date = datetime.datetime.strptime(date_str, '%m/%d/%Y %I:%M:%S %p')
        article_dates.append(article_date)

    return article_dates

def find_news_page_for_date(target_date, max_pages):
    left, right = 1, max_pages
    target_date = datetime.datetime.strptime(target_date, '%m/%d/%Y')

    while left <= right:
        mid = (left + right) // 2
        mid_dates = get_all_article_dates(mid)

        if not mid_dates:
            right = mid - 1
            continue

        for article_date in mid_dates:
            if article_date.date() == target_date.date():
                return mid

        if mid_dates[0].date() > target_date.date():
            left = mid + 1
        elif mid_dates[-1].date() < target_date.date():
            right = mid - 1

    return None

def pipeline_method(payload):
    tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
    model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
    classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
    result = classifier(payload)
    return result[0]

def get_news_by_date(df, target_date):
    df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%Y %I:%M:%S %p')
    filtered_df = df[df['datetime'].dt.strftime('%m/%d/%Y') == target_date]
    return filtered_df

def main(target_date, max_pages):
    page_number = find_news_page_for_date(target_date, max_pages)

    if not page_number:
        print(f'News for {target_date} not found within {max_pages} pages')
        return

    columns = ['datetime', 'title', 'source', 'top_sentiment', 'sentiment_score']
    df = pd.DataFrame(columns=columns)

    url = f'https://markets.businessinsider.com/news/aapl-stock?p={page_number}'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')

    articles = soup.find_all('div', class_='latest-news__story')

    for article in articles:
        dateTime = article.find('time', class_='latest-news__date').get('datetime')
        title = article.find('a', class_='news-link').text.strip()
        source = article.find('span', class_='latest-news__source').text.strip()

        output = pipeline_method(title)
        top_sentiment = output['label']
        sentiment_score = output['score']

        df = pd.concat([pd.DataFrame([[dateTime, title, source, top_sentiment, sentiment_score]], columns=df.columns), df], ignore_index=True)

    filtered_df = get_news_by_date(df, target_date)

    average_sentiment_score = filtered_df['sentiment_score'].mean()
    print(f"Average Sentiment Score for {target_date}: {average_sentiment_score}")

    filtered_df.to_csv('filtered_news.csv', index=False)

    from google.colab import files
    files.download('filtered_news.csv')

target_date = '03/31/2021'
max_pages = 200

main(target_date, max_pages)
